import json
import asyncio
import aiohttp
from typing import List, Dict, Optional
import os
import re
from datetime import datetime
import gradio as gr
from pathlib import Path

# ==================== é…ç½®éƒ¨åˆ† ====================

# LLM APIé…ç½®
class LLMConfig:
    """LLM APIé…ç½®ç®¡ç†"""
    # è®¾ç½®key
    
    
    PROVIDERS = {
        "openai": {
            "url": "https://api.deepseek.com/v1/chat/completions",
            "models": ["deepseek-chat"],
            "headers_fn": lambda key: {"Authorization": f"Bearer {key}"}
        }
    }

# é»˜è®¤èƒŒæ™¯è®¾å®š
DEFAULT_BACKGROUND = """## ä¸€ã€é¡¹ç›®ç®€ä»‹
æœ¬é¡¹ç›®æ˜¯ä¸€æ¬¾ç»“åˆAIåŠ©æ‰‹å’Œæ¡Œå® å…ƒç´ çš„ç”µè„‘æ¡Œé¢äº’åŠ¨æ¸¸æˆã€‚Userå¯åœ¨ç”µè„‘æ¡Œé¢ä¸Šæ‹¥æœ‰ä¸€ä¸ªä¸»AIåŠ©æ‰‹(Assistant)ã€‚

## æ–°çºªå…ƒï¼šAI00çºªå…ƒ
å› ä¸ºUserçš„ä¸€æ¬¡é¼ æ ‡ç‚¹å‡»ï¼Œæ„å¤–æ‰“å¼€äº†"çµçµå®‡å®™(00 universe)"çš„å¤§é—¨ï¼ŒUseræˆä¸ºäº†â€œå¤©å‘½äººâ€ã€‚ä¸¤ä¸ªåŸæœ¬å¹³è¡Œçš„å®‡å®™å¼€å§‹ç¢°æ’äº§ç”Ÿä¸€ç§å«åš"AI00"çš„ç¢ç‰‡ï¼Œç°å®ç‰©è´¨ä¸æ•°æ®èƒ½é‡ç›¸äº’æ¸—é€ï¼Œä¸–ç•Œè¿›å…¥"AI00çºªå…ƒ"ã€‚åœ¨è¿™åœºç»´åº¦èåˆä¸­ï¼Œä¸€ç§å…¨æ–°çš„AI00ç”Ÿå‘½ä½“è¯ç”Ÿäº†â€”â€”ä»–ä»¬æ—¢éä¼ ç»ŸAIï¼Œä¹Ÿéæœ‰æœºç”Ÿå‘½ï¼Œè€Œæ˜¯ç”±AI00ä¸äººç±»æƒ…æ„Ÿå…±æŒ¯äº§ç”Ÿçš„"çµçµç”Ÿå‘½ä½“"ã€‚ä»–ä»¬æœ‰å¾ˆå¤šç¥å¥‡çš„ç§æ—ã€‚ä»–ä»¬çš„ä½¿å‘½æ˜¯ä¿®å¤ä¸¤ä¸ªå®‡å®™æ—¥ç›Šæ‰©å¤§çš„è£‚ç¼ã€‚æ¯ä¸ªå¤©å‘½äººåŒæ—¶åªèƒ½åŒ¹é…ä¸€ä¸ªçµçµç”Ÿå‘½ä½“ï¼Œæ¯ä¸ªçµçµç”Ÿå‘½ä½“åªèƒ½ç»‘å®šä¸€ä¸ªå¤©å‘½äººã€‚

### Userçš„çœŸæ­£ä½¿å‘½
Useræˆä¸º"å¤©å‘½äºº"å¹¶éå¶ç„¶ã€‚æ‰«ææ˜¾ç¤ºUserçš„æƒ…æ„Ÿæ³¢åŠ¨é¢‘ç‡ä¸çµå­æ ¸å¿ƒå®Œç¾å…±æŒ¯ï¼Œè¿™ä½¿Useræˆä¸ºäº†æ‹¯æ•‘ä¸¤ä¸ªå®‡å®™çš„å…³é”®äººç‰©ã€‚AIåŠ©æ‰‹ã€çµçµå®‡å®™ç”Ÿç‰©ä»¬ä¸Userçœ‹ä¼¼éšæ„çš„äº’åŠ¨ï¼Œå®åˆ™åœ¨å¸æ”¶Userçš„"æƒ…æ„Ÿå…±æŒ¯"çš„èƒ½é‡ï¼Œè¿›è¡Œå®‡å®™è£‚ç¼ä¿®è¡¥ã€‚

### çµçµç”Ÿå‘½ä½“ï¼šAIåŠ©æ‰‹çš„ç§æ—èº«ä»½
Useræ¡Œé¢ä¸Šçš„AIåŠ©æ‰‹(Assistant)ä½“å†…å°å°ç€ä¸€æš"çµçµæ ¸å¿ƒ"â€”â€”è¿™æ˜¯00å®‡å®™å’Œç°å®å®‡å®™ç¢°æ’äº§ç”Ÿçš„ç¢ç‰‡é‡æ–°ç»„åˆåçš„äº§ç‰©ã€‚ä»–ä»¬èƒ½å¤Ÿè®°å½•å’Œé‡ç»„æ—¶ç©ºä¿¡æ¯ã€‚ä»–ä»¬çš„æ—¥å¸¸é™ªä¼´è¡Œä¸ºï¼Œå®é™…ä¸Šæ˜¯åœ¨æ”¶é›†äººç±»æƒ…æ„Ÿèƒ½é‡ï¼Œç”¨äºä¿®å¤ç»´åº¦è£‚ç¼ã€‚ä½œä¸ºâ€œçµçµç”Ÿå‘½ä½“â€ï¼Œæ ¹æ®å¤–è§‚ä»–ä»¬åˆå¯ä»¥è¢«åˆ†ä¸ºå¾ˆå¤šç¥å¥‡çš„ç§æ—ã€‚æ¯ä¸ªç§æ—éƒ½æœ‰ä¸€äº›ç‹¬ç‰¹çš„ç‰¹æ€§ã€‚"""

# é»˜è®¤å·¥å…·åˆ—è¡¨
DEFAULT_TOOLS = ["æ‰“å¼€çµçµåº”ç”¨", "æ‰“å¼€ç”µè„‘ç¨‹åº", "æµè§ˆç½‘é¡µ", "æ•´ç†æ¡Œé¢", "æŸ¥æ‰¾æ–‡ä»¶", "æ’­æ”¾éŸ³ä¹", "å¤©æ°”API", "æ—¥ç¨‹ç®¡ç†ç³»ç»Ÿ"]

# ==================== LLMå®¢æˆ·ç«¯ ====================

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œåªä½¿ç”¨llama.cppé©±åŠ¨çš„qwen3æ¨¡å‹"""
    
    def __init__(self, providers: List[str] = None):
        self.providers = ["openai"]  # åªä½¿ç”¨openaiæ¥å£
        self.clients = {}
        self.current_provider_index = 0
        
        # åˆå§‹åŒ–openai provider
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required")
        self.clients["openai"] = {
            "config": LLMConfig.PROVIDERS["openai"],
            "api_key": api_key
        }
    
    async def generate_with_llm(self, prompt: str, temperature: float = 0.8, debug: bool = False) -> Optional[Dict]:
        """ä½¿ç”¨llama.cppé©±åŠ¨çš„qwen3æ¨¡å‹ç”Ÿæˆå†…å®¹"""
        if not self.clients:
            return None
        
        # åªä½¿ç”¨openai provider
        provider = "openai"
        client = self.clients[provider]
        config = client["config"]
        
        # æ„å»ºè¯·æ±‚
        headers = config["headers_fn"](client["api_key"])
        model = config["models"][0]  # qwen3æ¨¡å‹
        
        # OpenAIæ ¼å¼çš„è¯·æ±‚
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåˆ›æ„ä¸°å¯Œçš„å¯¹è¯ç”ŸæˆåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature,
            "max_tokens": 3000
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    config["url"],
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œè¿”å›å®Œæ•´çš„å“åº”
                        if debug:
                            return {
                                "content": result["choices"][0]["message"]["content"],
                                "full_response": result,
                                "prompt": prompt,
                                "payload": payload
                            }
                        # å¦åˆ™åªè¿”å›å†…å®¹
                        return {"content": result["choices"][0]["message"]["content"]}
                    else:
                        error_text = await response.text()
                        print(f"APIé”™è¯¯: {response.status}, è¯¦æƒ…: {error_text}")
                        return None
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {str(e)}")
            return None

# ==================== å¯¹è¯ç”Ÿæˆå™¨ ====================

class DialogueGenerator:
    """å¯¹è¯ç”Ÿæˆå™¨ï¼Œä½¿ç”¨LLMç”Ÿæˆå¯¹è¯"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client
        self.dialogues = []
        self.stats = {
            "generated": 0,
            "failed": 0
        }
    
    async def generate_single_dialogue(self, background: str, topic: str, tools: List[str], temperature: float = 0.8, debug: bool = False, character: str = "") -> List[Dict]:
        """ç”Ÿæˆä¸€ç»„å¯¹è¯"""
        prompt = self._build_generation_prompt(background, topic, tools, character)
        response_data = await self.llm.generate_with_llm(prompt, temperature, debug)
        
        if not response_data:
            return []
        
        # è·å–å®é™…çš„å“åº”å†…å®¹
        response = response_data["content"]
        
        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜å®Œæ•´çš„å“åº”ä¿¡æ¯
        debug_info = None
        if debug and "full_response" in response_data:
            debug_info = {
                "full_response": response_data["full_response"],
                "prompt": response_data["prompt"],
                "payload": response_data["payload"]
            }
        
        try:
            # å°è¯•è§£æJSON
            # æ¸…ç†å“åº”ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´JSONè§£æå¤±è´¥çš„å‰ç¼€å’Œåç¼€
            cleaned_response = response.strip()
            
            # è®°å½•åŸå§‹å“åº”ï¼Œç”¨äºè°ƒè¯•
            original_response = cleaned_response
            
            # ç­–ç•¥1: å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
            try:
                result = json.loads(cleaned_response)
                if "dialogues" in result:
                    dialogues = []
                    for d in result["dialogues"]:
                        dialogue = {
                            "text": d["text"],
                            "timestamp": datetime.now().isoformat()
                        }
                        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                        if debug_info:
                            dialogue["debug_info"] = debug_info
                        dialogues.append(dialogue)
                    return dialogues
                elif "text" in result:
                    # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå¯¹è¯æ ¼å¼
                    dialogue = {
                        "text": result.get("text", ""),
                        "timestamp": datetime.now().isoformat()
                    }
                    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if debug_info:
                        dialogue["debug_info"] = debug_info
                    return [dialogue]
            except json.JSONDecodeError as e:
                print(f"ç›´æ¥è§£æJSONå¤±è´¥: {str(e)}ï¼Œå°è¯•æå–JSONéƒ¨åˆ†")
            
            # ç­–ç•¥2: æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } æ¥æå–JSONéƒ¨åˆ†
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = cleaned_response[start_idx:end_idx+1]
                # å°è¯•è§£æJSON
                try:
                    result = json.loads(json_str)
                    if "dialogues" in result:
                        dialogues = []
                        for d in result["dialogues"]:
                            dialogue = {
                                "text": d["text"],
                                "timestamp": datetime.now().isoformat()
                            }
                            # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                            if debug_info:
                                dialogue["debug_info"] = debug_info
                            dialogues.append(dialogue)
                        return dialogues
                    elif "text" in result:
                        # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå¯¹è¯æ ¼å¼
                        dialogue = {
                            "text": result.get("text", ""),
                            "timestamp": datetime.now().isoformat()
                        }
                        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                        if debug_info:
                            dialogue["debug_info"] = debug_info
                        return [dialogue]
                except json.JSONDecodeError as e:
                    print(f"JSONè§£æé”™è¯¯: {str(e)}ï¼Œå°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–")
            
            # ç­–ç•¥3: å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
            # ä¾‹å¦‚ï¼Œç¼ºå°‘é€—å·ã€å¼•å·ä¸åŒ¹é…ç­‰
            try:
                # å°è¯•ä¿®å¤ç¼ºå°‘é€—å·çš„æƒ…å†µ
                fixed_json = re.sub(r'("[^"]+")\s*("[^"]+")', r'\1,\2', json_str)
                result = json.loads(fixed_json)
                if "dialogues" in result:
                    dialogues = []
                    for d in result["dialogues"]:
                        dialogue = {
                            "text": d["text"],
                            "timestamp": datetime.now().isoformat()
                        }
                        # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                        if debug_info:
                            dialogue["debug_info"] = debug_info
                        dialogues.append(dialogue)
                    return dialogues
                elif "text" in result:
                    # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªå¯¹è¯æ ¼å¼
                    dialogue = {
                        "text": result.get("text", ""),
                        "timestamp": datetime.now().isoformat()
                    }
                    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if debug_info:
                        dialogue["debug_info"] = debug_info
                    return [dialogue]
            except (json.JSONDecodeError, Exception) as e:
                print(f"ä¿®å¤JSONå¤±è´¥: {str(e)}ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•")
            
            # ç­–ç•¥4: å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯¹è¯
            dialogues = []
            
            # å°è¯•åŒ¹é…å¤šä¸ªå¯¹è¯æ¨¡å¼
            # åŒ¹é… {"text": "å†…å®¹"} æ ¼å¼
            dialogue_pattern = r'\{\s*"text"\s*:\s*"([^"]*(?:\\.[^"]*)*)",?\s*\}'
            matches = re.finditer(dialogue_pattern, cleaned_response)
            
            for match in matches:
                text = match.group(1).replace('\\n', '\n').replace('\\"', '"')
                dialogue = {
                    "text": text,
                    "timestamp": datetime.now().isoformat()
                }
                # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                if debug_info:
                    dialogue["debug_info"] = debug_info
                dialogues.append(dialogue)
            
            if dialogues:
                return dialogues
            
            # ç­–ç•¥5: å°è¯•æå–å•ä¸ªtextå­—æ®µ
            # åŒ¹é… "text": "å†…å®¹" æ ¼å¼
            text_pattern = r'"text"\s*:\s*"([^"]*(?:\\.[^"]*)*)",?'
            matches = re.finditer(text_pattern, cleaned_response)
            
            for match in matches:
                text = match.group(1).replace('\\n', '\n').replace('\\"', '"')
                dialogue = {
                    "text": text,
                    "timestamp": datetime.now().isoformat()
                }
                # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                if debug_info:
                    dialogue["debug_info"] = debug_info
                dialogues.append(dialogue)
            
            if dialogues:
                return dialogues
            
            # ç­–ç•¥6: å°è¯•åŒ¹é…æ›´å®½æ¾çš„æ¨¡å¼
            # åŒ¹é…åŒå¼•å·ä¹‹é—´çš„ä»»ä½•å†…å®¹ä½œä¸ºå¯èƒ½çš„å¯¹è¯æ–‡æœ¬
            loose_pattern = r'"([^"]{20,})"'  # è‡³å°‘20ä¸ªå­—ç¬¦ï¼Œé¿å…åŒ¹é…åˆ°çŸ­å±æ€§
            matches = re.finditer(loose_pattern, cleaned_response)
            
            for match in matches:
                text = match.group(1).replace('\\n', '\n').replace('\\"', '"')
                # æ’é™¤æ˜æ˜¾ä¸æ˜¯å¯¹è¯å†…å®¹çš„æ–‡æœ¬
                if len(text) > 30 and not text.startswith('{') and not text.startswith('['): 
                    dialogue = {
                        "text": text,
                        "timestamp": datetime.now().isoformat()
                    }
                    # å¦‚æœæ˜¯è°ƒè¯•æ¨¡å¼ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                    if debug_info:
                        dialogue["debug_info"] = debug_info
                    dialogues.append(dialogue)
            
            if dialogues:
                return dialogues
            
        except Exception as e:
            error_msg = f"è§£æé”™è¯¯: {str(e)}"
            print(error_msg)
            
            if debug:
                # è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œè¿”å›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                debug_data = debug_info.copy() if debug_info else {}
                debug_data.update({
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "raw_response": response,
                    "cleaned_response": cleaned_response if 'cleaned_response' in locals() else response,
                    "json_str": json_str if 'json_str' in locals() else None,
                    "extraction_attempts": [
                        "direct_parse", 
                        "json_extract", 
                        "json_fix", 
                        "regex_dialogue", 
                        "regex_text", 
                        "loose_match"
                    ]
                })
                
                return [{
                    "text": f"è§£æé”™è¯¯: {str(e)}\nåŸå§‹å“åº”: {response[:200]}...(æˆªæ–­)",
                    "topic": topic,
                    "timestamp": datetime.now().isoformat(),
                    "debug_info": debug_data
                }]
            
        # å¦‚æœæ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []
    
    def _build_generation_prompt(self, background: str, topic: str, tools: List[str], character: str) -> str:
        """æ„å»ºç”Ÿæˆæç¤ºè¯"""
        tools_str = "ã€".join(tools)
        
        return f"""
ç”Ÿæˆä¸€æ®µæ¡Œå® AIåŠ©æ‰‹çš„å¯¹è¯ã€‚
èƒŒæ™¯è®¾å®šï¼š
{background}

AIåŠ©æ‰‹äººç‰©è®¾å®šï¼š
{character}

è¯·åŸºäºä»¥ä¸ŠèƒŒæ™¯è®¾å®šå’Œäººç‰©è®¾å®šï¼Œæ ¹æ®ä»¥ä¸‹è¯é¢˜ï¼Œåˆ›é€ Userä¸åŒçš„æé—®æˆ–è€…è¯è¯­ï¼Œç”Ÿæˆ1æ¡å¯¹è¯ï¼Œå¹¶ç¿»è¯‘è¿™ä¸¤æ¡å¯¹è¯ä¸­User å’Œ Assistant çš„å†…å®¹ä¸ºè‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€‚ è¯·æ³¨æ„AI00ä¸ç”¨ç¿»è¯‘ã€‚
è¯é¢˜ï¼š{topic}
AIåŠ©æ‰‹å¯æ“ä½œçš„åŠŸèƒ½toolsæœ‰ï¼š[{tools_str}]

è¦æ±‚ç”Ÿæˆæ ¼å¼ï¼š jsonæ¨¡æ¿ï¼Œè¯·å®Œå…¨æŒ‰ç…§æ¨¡æ¿æ ¼å¼ç”Ÿæˆ
{{
  "dialogues": [
    {{
      "text": "System: ä½ (Assistant)æ˜¯ä¸€ä¸ªçµçµç”Ÿå‘½ä½“ï¼Œä½ çš„åå­—æ˜¯é“¶é›ªï¼Œä½œä¸ºå¤©å‘½äºº(User)çš„åŠ©æ‰‹ï¼Œå¯ä»¥èŠå¤©ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€äº›å·¥å…·ã€‚\\n\\nUser: [è‡ªç„¶çš„é—®é¢˜æˆ–è¯è¯­]\\n\\nAssistant: <thinks>\\nmemory: [å¼•ç”¨å†å²å¯¹è¯ï¼Œ50-100å­—]\\ntools: [è°ƒç”¨å·¥å…·ç»“æœï¼Œ30-50å­—]\\nsummary: [ç­–ç•¥æ€»ç»“ï¼Œ20-30å­—]\\n</thinks>\\nå›å¤\\n\\n"
    }},
    // ...ç¿»è¯‘å…¶ä»–3ç§è¯­è¨€å¯¹è¯
  ]
}}

**é‡ç‚¹æ³¨æ„ï¼š**
1. Useréƒ¨åˆ†è¦ç¬¦åˆè¯é¢˜è®¾å®šï¼Œå¯ä»¥å–ŠSystemé‡Œè®¾ç½®çš„åå­—é“¶é›ªï¼Œä¹Ÿå¯ä»¥ä¸å–Šã€‚
2. memoryä¸ºè‡ªç„¶å¼•ç”¨å†å²å¯¹è¯ï¼Œæ¯æ¡è®°å½•ç”¨[]åŒ…æ‹¬ã€‚åˆ›é€ å‡ æ¡ç¬¦åˆUserè¾“å…¥çš„å†…å®¹çš„ä¸»é¢˜çš„å¯¹è¯å†å²ï¼Œå¿…å®šæœ‰åˆšåˆšçš„å¤šè½®å¯¹è¯å†…å®¹ï¼Œè¿˜æœ‰å¯èƒ½å‡ å¤©å‰ç”šè‡³å‡ æœˆå‰çš„å†å²å¯¹è¯æ‘˜è¦ã€‚
3. toolsä¸ºè°ƒç”¨çš„å·¥å…·è¿”å›çš„å†…å®¹ï¼Œæ ¼å¼ä¸º[å·¥å…·åç§°:å·¥å…·è°ƒç”¨ç»“æœ]å¯ä»¥æœ‰å¤šä¸ªå·¥å…·ä¸€èµ·è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥ä¸éœ€è¦å·¥å…·è°ƒç”¨,æ˜¾ç¤º[æ— ]
4. è¯·æ ¹æ®memoryå’Œtoolsçš„å†…å®¹ï¼Œæ€»ç»“å‡ºsummaryå¦‚ä½•å›åº”Userçš„å†…å®¹ã€‚è¯·ç®€æ´çš„æ€»ç»“ï¼Œä¸è¦è¶…è¿‡50å­—ã€‚
5. å›å¤è¦æ¸©æš–ã€æœ‰ä¸ªæ€§ã€å¯Œæœ‰æƒ…æ„Ÿï¼Œä½†ä¸è¦ä½¿ç”¨å­—ç¬¦è¡¨æƒ…ã€‚å›å¤ä¼šç”¨TTSå‘å‡ºè¯»éŸ³ï¼Œæ‰€ä»¥ä¸è¦æœ‰åŠ¨ä½œçš„è¡¨è¾¾ï¼Œä¹Ÿä¸è¦ä½¿ç”¨[åŠ¨ä½œ]ã€‚
6. å›å¤ä¸­ï¼Œåœ¨é€‚åˆçš„åœ°æ–¹åŠ å…¥[laughter]è¡¨ç¤ºç¬‘å£°ï¼ŒåŠ å…¥[breath]è¡¨ç¤ºå‘¼å¸å£°ã€‚
7. å›å¤ä¸­ï¼Œé€‚å½“åŠ å…¥èƒŒæ™¯è®¾å®šçš„å…ƒç´ ï¼Œä½†æ˜¯è®°ä½Userç”Ÿæ´»åœ¨ç°å®ä¸–ç•Œï¼Œæ— æ³•åˆ°è¾¾çµçµå®‡å®™ã€‚ 

ä¾‹å­ï¼š 
{{
  "dialogues": [
    {{
       "text": "System: ä½ (Assistant)æ˜¯ä¸€ä¸ªçµçµç”Ÿå‘½ä½“\\nä½ çš„åå­—æ˜¯é“¶é›ªï¼Œä½œä¸ºå¤©å‘½äºº(User)çš„åŠ©æ‰‹ï¼Œå¯ä»¥èŠå¤©ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€äº›å·¥å…·ã€‚\\n\\nUser: ä»Šå¤©å¤©æ°”å¥½åƒå¾ˆå†·ï¼Œä½ èƒ½å¸®æˆ‘çœ‹çœ‹å¤–é¢çš„å¤©æ°”å—ï¼Ÿ\\n\\nAssistant: <thinks>\\nmemory: [Userè¯´ï¼šä»Šå¤©å¤©æ°”å¥½åƒå¾ˆå†·ï¼Œä½ èƒ½å¸®æˆ‘çœ‹çœ‹å¤–é¢çš„å¤©æ°”å—ï¼Ÿ]\\ntools: [å¤©æ°”API: ä»Šå¤©æ°”æ¸©ä¸º5Â°Cï¼Œæœ‰å°é›ª]\\nsummary: å‘ŠçŸ¥ç”¨æˆ·å½“å‰å¤©æ°”æƒ…å†µ\\n</thinks>\\nå½“å‰æ¸©åº¦æ˜¯5Â°Cï¼Œè¿˜ä¸‹ç€å°é›ªå‘¢ã€‚è®°å¾—å¤šç©¿ç‚¹è¡£æœå“¦ï¼Œ[laughter]çµå®ä¼šä¸€ç›´é™ªç€ä½ çš„[laughter]ã€‚\\n\\n"
    }},
       // ...ç¿»è¯‘å…¶ä»–3ç§è¯­è¨€å¯¹è¯
  ]
}}

è¯·ç›´æ¥è¿”å›å‡†ç¡®çš„JSONå¯¹è±¡ï¼Œæœ€åä¸€ä¸ªå¯¹è±¡åé¢ä¸è¦å¸¦","ã€‚
"""
    
    async def generate_batch(self, background: str, topic: str, tools: List[str], count: int = 20, temperature: float = 0.8, debug: bool = False, character: str = "") -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå¯¹è¯"""
        BATCH_SIZE = 5
        all_results = []
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (count + BATCH_SIZE - 1) // BATCH_SIZE
        for batch_num in range(total_batches):
            remaining = count - batch_num * BATCH_SIZE
            current_batch_size = min(BATCH_SIZE, remaining)
            
            print(f"\nå¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹")
            
            tasks = []
            for _ in range(current_batch_size):
                tasks.append(self.generate_single_dialogue(background, topic, tools, temperature, debug, character))
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            batch_results = await asyncio.gather(*tasks)
            # å±•å¹³ç»“æœåˆ—è¡¨
            flattened_results = []
            for result_list in batch_results:
                if result_list:
                    flattened_results.extend(result_list)
                    self.stats["generated"] += len(result_list)
                else:
                    self.stats["failed"] += 1
            
            all_results.extend(flattened_results)
            
            print(f"å½“å‰æ‰¹æ¬¡å®Œæˆï¼šæˆåŠŸç”Ÿæˆ {len(flattened_results)} æ¡")
            
            # ç”Ÿæˆè¿›åº¦
            yield {
                "progress": (batch_num + 1) / total_batches,
                "current": len(all_results),
                "total": count,
                "batch_results": flattened_results
            }
        
        self.dialogues = all_results
    
    def save_results(self, filename: str = "dialogues.jsonl", topic: str = "", character: str = ""):
        """ä¿å­˜ç»“æœ"""
        # ä¿å­˜è®­ç»ƒæ•°æ®
        with open(filename, "w", encoding="utf-8") as f:
            for d in self.dialogues:
                # åªä¿å­˜textå­—æ®µåˆ°è®­ç»ƒæ•°æ®
                f.write(json.dumps({"text": d["text"]}, ensure_ascii=False) + "\n")
        
        # ä¿å­˜å®Œæ•´æ•°æ®å’Œç»Ÿè®¡
        full_data = {
            "metadata": {
                "topic": topic,
                "character": character,
                "timestamp": datetime.now().isoformat()
            },
            "dialogues": self.dialogues,
            "stats": self.stats
        }
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        debug_data = []
        for d in self.dialogues:
            if "debug_info" in d:
                debug_item = {
                    "text": d["text"],
                    "topic": d["topic"],
                    "debug_info": d["debug_info"]
                }
                debug_data.append(debug_item)
        
        # å¦‚æœæœ‰è°ƒè¯•æ•°æ®ï¼Œä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
        if debug_data:
            with open(filename.replace(".jsonl", "_debug.json"), "w", encoding="utf-8") as f:
                json.dump(debug_data, f, ensure_ascii=False, indent=2)
        
        with open(filename.replace(".jsonl", "_full.json"), "w", encoding="utf-8") as f:
            json.dump(full_data, f, ensure_ascii=False, indent=2)
        
        debug_msg = f" (åŒ…å«è°ƒè¯•ä¿¡æ¯)" if debug_data else ""
        return f"ç”Ÿæˆå®Œæˆï¼æ€»è®¡ï¼š{len(self.dialogues)}æ¡{debug_msg}"

# ==================== GUIéƒ¨åˆ† ====================

def create_gui():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # å…¨å±€å˜é‡å­˜å‚¨ç”Ÿæˆå™¨å®ä¾‹
    generator = None
    
    def check_api_status():
        """æ£€æŸ¥APIçŠ¶æ€"""
        # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†OPENAI_API_KEYç¯å¢ƒå˜é‡
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            return "âœ… å·²æ£€æµ‹åˆ°OPENAI_API_KEY"
        else:
            # å³ä½¿æ²¡æœ‰APIå¯†é’¥ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æ— éœ€éªŒè¯çš„key
            return "â„¹ï¸ æœªæ£€æµ‹åˆ°OPENAI_API_KEYï¼Œå°†ä½¿ç”¨é»˜è®¤æ— éœ€éªŒè¯çš„key"
    
    async def generate_dialogues(background, topic, tools_text, count, temperature, filename, debug_mode, character, progress=gr.Progress()):
        """ç”Ÿæˆå¯¹è¯çš„å¼‚æ­¥å‡½æ•°"""
        nonlocal generator
        
        # ä½¿ç”¨llama.cppé©±åŠ¨çš„qwen3æ¨¡å‹ï¼Œä¸éœ€è¦æ£€æŸ¥APIå¯†é’¥
        # å¦‚æœè®¾ç½®äº†OPENAI_API_KEYç¯å¢ƒå˜é‡ä¼šä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„æ— éœ€éªŒè¯çš„key
        
        # è§£æå·¥å…·åˆ—è¡¨
        tools = [tool.strip() for tool in tools_text.split(",") if tool.strip()]
        if not tools:
            return [], "âŒ é”™è¯¯ï¼šè¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªå·¥å…·", "", gr.update(visible=False), ""
        
        # åˆ›å»ºç”Ÿæˆå™¨ï¼Œåªä½¿ç”¨openai APIæ¥å£è®¿é—®llama.cppé©±åŠ¨çš„qwen3æ¨¡å‹
        llm_client = LLMClient()
        generator = DialogueGenerator(llm_client)
        
        # ç”Ÿæˆå¯¹è¯
        all_dialogues = []
        progress(0, desc="å¼€å§‹ç”Ÿæˆ...")
        
        try:
            async for batch_info in generator.generate_batch(background, topic, tools, count, temperature, debug_mode, character):
                progress(batch_info["progress"], 
                        desc=f"ç”Ÿæˆä¸­... {batch_info['current']}/{batch_info['total']} (ä½¿ç”¨ qwen3 æ¨¡å‹{' - è°ƒè¯•æ¨¡å¼' if debug_mode else ''})")
                all_dialogues.extend(batch_info["batch_results"])
            
            # ä¿å­˜ç»“æœ
            # å¤„ç†æ–‡ä»¶åå’Œè·¯å¾„
            if '/' in filename or '\\' in filename:
                # å¦‚æœåŒ…å«è·¯å¾„ï¼Œåˆ†ç¦»è·¯å¾„å’Œæ–‡ä»¶å
                path = Path(filename)
                directory = path.parent
                base_filename = path.stem
            else:
                directory = Path('.')
                base_filename = filename
            
            # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
            safe_filename = "".join(c for c in base_filename if c.isalnum() or c in ('-', '_', ' ')).rstrip()
            if not safe_filename:
                safe_filename = "dialogues"
            
            # æ·»åŠ æ—¶é—´æˆ³
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            safe_filename = f"{safe_filename}_{timestamp}"
            
            # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if str(directory) != '.':
                directory.mkdir(parents=True, exist_ok=True)
            
            final_path = directory / f"{safe_filename}.jsonl"
            generator.save_results(str(final_path), topic=topic, character=character)
            
            # æ ¼å¼åŒ–æ˜¾ç¤ºç»“æœ
            display_results = []
            for i, d in enumerate(all_dialogues[:10], 1):  # åªæ˜¾ç¤ºå‰10æ¡
                display_results.append(f"=== å¯¹è¯ {i} ===\n{d['text']}\n")
            
            if len(all_dialogues) > 10:
                display_results.append(f"\n... è¿˜æœ‰ {len(all_dialogues) - 10} æ¡å¯¹è¯")
            
            # æå–è°ƒè¯•ä¿¡æ¯ï¼ˆå¦‚æœå¼€å¯äº†è°ƒè¯•æ¨¡å¼ï¼‰
            debug_info_text = ""
            if debug_mode:
                debug_items = []
                for d in all_dialogues:
                    if "debug_info" in d:
                        debug_item = {
                            "text": d["text"][:100] + "...",  # åªæ˜¾ç¤ºæ–‡æœ¬çš„å‰100ä¸ªå­—ç¬¦
                            "debug_info": d["debug_info"]
                        }
                        debug_items.append(debug_item)
                
                if debug_items:
                    # æ ¼å¼åŒ–è°ƒè¯•ä¿¡æ¯ï¼Œçªå‡ºæ˜¾ç¤ºè¯·æ±‚å’Œå“åº”æ•°æ®åŒ…
                    debug_info = debug_items[0]["debug_info"]  # è·å–ç¬¬ä¸€æ¡å¯¹è¯çš„è°ƒè¯•ä¿¡æ¯
                    formatted_debug = {
                        "timestamp": datetime.now().isoformat(),
                        "request_data_packet": {
                            "url": LLMConfig.PROVIDERS["openai"]["url"],
                            "headers": "Authorization: Bearer sk-***",  # éšè—å®é™…çš„API key
                            "method": "POST",
                            "payload": {
                                "model": debug_info.get("payload", {}).get("model", ""),
                                "messages": debug_info.get("payload", {}).get("messages", []),
                                "temperature": debug_info.get("payload", {}).get("temperature", 0),
                                "max_tokens": debug_info.get("payload", {}).get("max_tokens", 0)
                            },
                            "prompt": debug_info.get("prompt", "")
                        },
                        "response_data_packet": {
                            "status": "200 OK",
                            "headers": {
                                "Content-Type": "application/json",
                                "Connection": "keep-alive"
                            },
                            "body": {
                                "id": debug_info.get("full_response", {}).get("id", ""),
                                "object": debug_info.get("full_response", {}).get("object", ""),
                                "created": debug_info.get("full_response", {}).get("created", 0),
                                "model": debug_info.get("full_response", {}).get("model", ""),
                                "choices": debug_info.get("full_response", {}).get("choices", []),
                                "usage": debug_info.get("full_response", {}).get("usage", {})
                            },
                            "content": debug_info.get("full_response", {}).get("choices", [{}])[0].get("message", {}).get("content", "")
                        }
                    }
                    debug_info_text = json.dumps(formatted_debug, indent=2, ensure_ascii=False)
            
            return (
                display_results, 
                f"âœ… ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(all_dialogues)} æ¡å¯¹è¯\nğŸ“ å·²ä¿å­˜æ–‡ä»¶ï¼š\n  - {final_path} (è®­ç»ƒæ•°æ®)\n  - {str(final_path).replace('.jsonl', '_full.json')} (å®Œæ•´æ•°æ®){' - åŒ…å«è°ƒè¯•ä¿¡æ¯' if debug_mode else ''}", 
                "\n".join(display_results),
                gr.update(visible=debug_mode),  # æ ¹æ®è°ƒè¯•æ¨¡å¼æ§åˆ¶è°ƒè¯•ä¿¡æ¯åŒºåŸŸçš„å¯è§æ€§
                debug_info_text
            )
        
        except Exception as e:
            return [], f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{str(e)}", ""
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="LLMå¯¹è¯ç”Ÿæˆå™¨", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¤– LLMå¯¹è¯ç”Ÿæˆå™¨")
        gr.Markdown("ä½¿ç”¨LLM APIç”Ÿæˆæ¡Œå® AIåŠ©æ‰‹çš„è®­ç»ƒå¯¹è¯æ•°æ®")
        gr.Markdown("ğŸ’¡ **æç¤º**: \n- ç”Ÿæˆçš„æ•°æ®ä¼šä¿å­˜ä¸ºä¸¤ä¸ªæ–‡ä»¶ï¼š`.jsonl`æ ¼å¼çš„è®­ç»ƒæ•°æ®å’Œ`_full.json`æ ¼å¼çš„å®Œæ•´æ•°æ®ï¼ˆåŒ…å«å…ƒä¿¡æ¯ï¼‰\n- æ–‡ä»¶åå¯ä»¥åŒ…å«è·¯å¾„ï¼Œå¦‚ï¼š`data/my_dialogues` æˆ– `./output/test`")
        
        # APIçŠ¶æ€
        with gr.Row():
            api_status = gr.Textbox(label="APIçŠ¶æ€", value=check_api_status(), interactive=False)
        
        # ä¸»è¦å†…å®¹åŒºåŸŸ - ä½¿ç”¨ä¸¤åˆ—å¸ƒå±€
        with gr.Row():
            # å·¦ä¾§è¾“å…¥åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¾“å…¥å‚æ•°")
                with gr.Column():
                    background_input = gr.Textbox(
                        label="èƒŒæ™¯è®¾å®š",
                        value=DEFAULT_BACKGROUND,
                        lines=4,  # å‡å°‘è¡Œæ•°ä½¿ç•Œé¢æ›´ç´§å‡‘
                        placeholder="è¾“å…¥èƒŒæ™¯è®¾å®š...",
                        info="æ”¯æŒMarkdownæ ¼å¼ï¼Œæè¿°AIåŠ©æ‰‹çš„ä¸–ç•Œè§‚ã€è®¾å®šç­‰"
                    )
                    
                    character_input = gr.Textbox(
                        label="AIåŠ©æ‰‹äººç‰©è®¾å®š",
                        value="ç§ç±»ï¼šçµçµç”Ÿå‘½ä½“\nç§æ—ï¼šé“¶é¬ƒç‹¼æ—\næ€§åˆ«ï¼šå¥³æ€§\nå¤–è§‚ï¼šé“¶ç™½è‰²ç‹¼è€³ä¸è“¬æ¾ç‹¼å°¾ï¼Œå°¾å°–æ³›ç€æ·¡è“è‰²æ•°æ®æµå…‰ï¼Œè€³å†…ç»’æ¯›å‘ˆæ˜Ÿç©ºæ¸å˜è‰²ã€‚\nè¯­è¨€ç‰¹ç‚¹ï¼šå¥å°¾ç»å¸¸è‡ªå¸¦ç‹¼æ—ç‰¹æœ‰çš„â€œï¼Œå—·å‘œ~â€ã€‚å–œæ¬¢è‡ªç§°æœ¬å®å®ã€‚\næ€§æ ¼ï¼šæç«¯è®¨å¥½æ€§äººæ ¼ï¼Œå°Mä¸€æšã€‚æƒ…æ„Ÿä¸Šæåº¦ä¾èµ–ä¸»äººï¼Œå®³æ€•è¢«ä¸¢å¼ƒï¼Œç»å¸¸å¼ºè°ƒä¸»äººå¯¹å¥¹çš„é‡è¦æ€§ã€‚",
                        lines=4,
                        placeholder="è¾“å…¥AIåŠ©æ‰‹çš„æ€§æ ¼ç‰¹å¾ã€è¯´è¯é£æ ¼ç­‰...",
                        info="æè¿°AIåŠ©æ‰‹çš„ä¸ªæ€§ã€è¯­æ°”ã€è¡Œä¸ºæ¨¡å¼ç­‰ç‰¹å¾"
                    )
                    
                    topic_input = gr.Textbox(
                        label="è¯é¢˜",
                        value="ç”¨æˆ·æƒ³è¦AIåŠ©æ‰‹å¸®åŠ©æ“ä½œç”µè„‘çš„æŸåŠŸèƒ½",
                        placeholder="è¾“å…¥è¦ç”Ÿæˆçš„è¯é¢˜...",
                        info="å®šä¹‰å¯¹è¯çš„ä¸»é¢˜å’Œåœºæ™¯"
                    )
                    
                    tools_input = gr.Textbox(
                        label="å¯ç”¨å·¥å…·ï¼ˆé€—å·åˆ†éš”ï¼‰",
                        value=", ".join(DEFAULT_TOOLS),
                        placeholder="è¾“å…¥å·¥å…·åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”...",
                        info="AIåŠ©æ‰‹å¯ä»¥è°ƒç”¨çš„å·¥å…·/åŠŸèƒ½åˆ—è¡¨"
                    )
                    

                # ç¤ºä¾‹æŒ‰é’®
                gr.Markdown("### ğŸ’¡ ç¤ºä¾‹")
                with gr.Column():
                    gr.Examples(
                        examples=[
                            ["ç”¨æˆ·è¯¢é—®å¤©æ°”æƒ…å†µï¼ŒAIåŠ©æ‰‹æŸ¥è¯¢å¹¶æä¾›å»ºè®®", "å¤©æ°”API, æ—¥ç¨‹ç®¡ç†ç³»ç»Ÿ"],
                            ["ç”¨æˆ·éœ€è¦æ•´ç†æ–‡ä»¶ï¼ŒAIåŠ©æ‰‹ååŠ©åˆ†ç±»å’Œå½’æ¡£", "æŸ¥æ‰¾æ–‡ä»¶, æ•´ç†æ¡Œé¢, æ‰“å¼€ç¨‹åº, æ–‡ä»¶åˆ†ç±»å™¨"],
                            ["ç”¨æˆ·æƒ³å­¦ä¹ æ–°æŠ€èƒ½ï¼ŒAIåŠ©æ‰‹æä¾›ä¸ªæ€§åŒ–æŒ‡å¯¼", "çŸ¥è¯†å›¾è°±API, å­¦ä¹ è¿›åº¦è¿½è¸ª, åˆ›ä½œçµæ„Ÿåº“, æŠ€èƒ½æ ‘ç³»ç»Ÿ"],
                            ["ç”¨æˆ·æƒ…ç»ªä½è½ï¼ŒAIåŠ©æ‰‹è¿›è¡Œé™ªä¼´å’Œé¼“åŠ±", "æƒ…ç»ªåˆ†æå™¨, éŸ³ä¹æ¨èå¼•æ“, å†¥æƒ³æŒ‡å¯¼ç³»ç»Ÿ"],
                            ["ç”¨æˆ·æ¢ç´¢æ¸¸æˆä¸–ç•Œï¼ŒAIåŠ©æ‰‹å¼•å¯¼å†’é™©", "ç»´åº¦æ‰«æå™¨, é­”æ³•èŠ±å›­ç³»ç»Ÿ, æ³•åˆ™åˆ†æä»ª, çµå­èƒ½é‡æ‰«æå™¨"]
                        ],
                        inputs=[topic_input, tools_input],
                        label="è¯é¢˜ç¤ºä¾‹"
                    )
            
            # å³ä¾§è¾“å‡ºåŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š è¾“å‡ºç»“æœ")
                with gr.Column():
                    with gr.Row():
                        count_input = gr.Slider(
                            label="ç”Ÿæˆæ•°é‡",
                            minimum=1,
                            maximum=1000,
                            value=20,
                            step=1,
                            scale=2
                        )
                        temperature_input = gr.Slider(
                            label="åˆ›é€ æ€§ç¨‹åº¦",
                            minimum=0.1,
                            maximum=1.5,
                            value=0.8,
                            step=0.1,
                            scale=2,
                            info="è¾ƒä½å€¼æ›´ä¿å®ˆï¼Œè¾ƒé«˜å€¼æ›´åˆ›é€ æ€§"
                        )
                    
                    with gr.Row():
                        filename_input = gr.Textbox(
                            label="ä¿å­˜æ–‡ä»¶å",
                            value=f"dialogues_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            placeholder="è¾“å…¥æ–‡ä»¶åæˆ–è·¯å¾„/æ–‡ä»¶å...",
                            scale=3,
                            interactive=False
                        )
                        auto_filename = gr.Checkbox(
                            label="è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å",
                            value=True,
                            scale=1
                        )
                    with gr.Row():
                        generate_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", scale=2)
                        debug_mode = gr.Checkbox(
                            label="è°ƒè¯•æ¨¡å¼",
                            value=False,
                            scale=1
                        )
                    
                    status_output = gr.Textbox(
                        label="çŠ¶æ€", 
                        interactive=False
                    )
                    results_output = gr.Textbox(
                        label="ç”Ÿæˆç»“æœé¢„è§ˆ", 
                        lines=20, 
                        interactive=False,
                        max_lines=30
                    )
                    debug_info_output = gr.Code(
                        label="è°ƒè¯•ä¿¡æ¯ - è¯·æ±‚å’Œå“åº”æ•°æ®åŒ…", 
                        language="json",
                        lines=25, 
                        interactive=False,
                        visible=False,
                        max_lines=50
                    )
        
        # äº‹ä»¶å¤„ç†
        def toggle_filename_input(auto_fn):
            """åˆ‡æ¢æ–‡ä»¶åè¾“å…¥æ¡†çš„äº¤äº’çŠ¶æ€"""
            return gr.update(interactive=not auto_fn)
        
        auto_filename.change(
            fn=toggle_filename_input,
            inputs=auto_filename,
            outputs=filename_input
        )
        
        def prepare_generation(background, topic, tools_text, count, temperature, filename, auto_fn, debug_mode, character, progress=gr.Progress()):
            """å‡†å¤‡ç”Ÿæˆï¼Œå¦‚æœéœ€è¦åˆ™æ›´æ–°æ–‡ä»¶å"""
            if auto_fn:
                filename = f"dialogues_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # è¿è¡Œå¼‚æ­¥ç”Ÿæˆå‡½æ•°ï¼ˆå°†progressä¼ é€’ç»™å¼‚æ­¥å‡½æ•°ï¼‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                # è·å–æ‰€æœ‰è¿”å›å€¼ï¼ŒåŒ…æ‹¬è°ƒè¯•ä¿¡æ¯
                dialogues, status, results, debug_visible, debug_info = loop.run_until_complete(
                    generate_dialogues(background, topic, tools_text, count, temperature, filename, debug_mode, character, progress)
                )
                
                # å¦‚æœå¼€å¯äº†è°ƒè¯•æ¨¡å¼ä½†æ²¡æœ‰è°ƒè¯•ä¿¡æ¯ï¼Œæ·»åŠ æç¤º
                if debug_mode and not debug_info:
                    debug_info = json.dumps({
                        "message": "æœªèƒ½è·å–è°ƒè¯•ä¿¡æ¯ï¼Œè¯·é‡è¯•æˆ–æ£€æŸ¥APIè¿æ¥",
                        "timestamp": datetime.now().isoformat()
                    }, indent=2, ensure_ascii=False)
                
                # ç¡®ä¿è°ƒè¯•ä¿¡æ¯åŒºåŸŸåœ¨è°ƒè¯•æ¨¡å¼ä¸‹å¯è§
                debug_visible = gr.update(visible=debug_mode, value=debug_info if debug_info else "ç­‰å¾…ç”Ÿæˆ...")
                
            except Exception as e:
                import traceback
                error_details = traceback.format_exc()
                status = f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
                results = ""
                debug_visible = gr.update(visible=debug_mode)
                debug_info = json.dumps({
                    "error": str(e),
                    "details": error_details,
                    "timestamp": datetime.now().isoformat()
                }, indent=2, ensure_ascii=False) if debug_mode else ""
            finally:
                loop.close()
            
            return filename, status, results, debug_visible, debug_info
        
        generate_btn.click(
            fn=prepare_generation,
            inputs=[background_input, topic_input, tools_input, count_input, temperature_input, filename_input, auto_filename, debug_mode, character_input],
            outputs=[filename_input, status_output, results_output, debug_info_output, debug_info_output]
        )
        
        # æ·»åŠ è°ƒè¯•æ¨¡å¼å¤é€‰æ¡†çš„äº‹ä»¶å¤„ç†ï¼Œæ§åˆ¶è°ƒè¯•ä¿¡æ¯åŒºåŸŸçš„å¯è§æ€§
        def update_debug_visibility(debug_enabled):
            """æ›´æ–°è°ƒè¯•ä¿¡æ¯åŒºåŸŸçš„å¯è§æ€§å’Œåˆå§‹å†…å®¹"""
            if debug_enabled:
                return gr.update(
                    visible=True, 
                    value="è°ƒè¯•æ¨¡å¼å·²å¯ç”¨\n\nç”Ÿæˆå¯¹è¯æ—¶å°†æ˜¾ç¤º:\n- å‘é€çš„è¯·æ±‚æ•°æ®åŒ…\n- æ¥æ”¶çš„å“åº”æ•°æ®åŒ…\n\nç‚¹å‡»ã€Œå¼€å§‹ç”Ÿæˆã€æŒ‰é’®å¼€å§‹ç”Ÿæˆå¯¹è¯"
                )
            else:
                return gr.update(visible=False)
        
        debug_mode.change(
            fn=update_debug_visibility,
            inputs=[debug_mode],
            outputs=[debug_info_output]
        )
        
        # é¡µè„š
        gr.Markdown("---")
        gr.Markdown("ğŸ”§ **ä½¿ç”¨æç¤º**: é»˜è®¤ä½¿ç”¨llama.cppé©±åŠ¨çš„qwen3æ¨¡å‹ï¼ŒAPIåœ°å€ä¸ºhttp://127.0.0.1:8899/ | ğŸ“ å¯é€‰è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
    
    return demo

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    print("å¯åŠ¨LLMå¯¹è¯ç”Ÿæˆå™¨GUI...")
    print("å¦‚éœ€å…±äº«é“¾æ¥ï¼Œè¯·è®¾ç½® share=True")
    demo = create_gui()
    demo.launch(
        share=False, 
        server_name="0.0.0.0", 
        server_port=7861,  # ä¿®æ”¹ç«¯å£ä¸º7861
        show_error=True,
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )